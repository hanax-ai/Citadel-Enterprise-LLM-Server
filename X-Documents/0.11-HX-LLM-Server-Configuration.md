# üõ∞Ô∏è Citadel Alpha Infrastructure ‚Äì Final Configuration Report  
**Node:** `hx-llm-server-01`  
**Role:** Primary LLM Inference Server  
**Version:** 1.0.0  
**Author:** Hana-X Infrastructure Engineering  
**Date:** 2025-07-19  

---

## 1. üéØ Purpose

This document captures the authoritative configuration state of `hx-llm-server-01`, the first operational node in the Citadel Alpha LLM cluster. It defines hardware, software, storage, GPU, model management, runtime services, and validation results. This node is optimized for large-scale model serving under the vLLM runtime and is the benchmark for all future replication and scaling efforts within the Citadel Alpha initiative.

---

## 2. üñ•Ô∏è Hardware & Network Configuration

| Component           | Details                                    |
|---------------------|--------------------------------------------|
| CPU                 | AMD Ryzen Threadripper PRO 7965WX          |
| RAM                 | 256 GB DDR5 ECC                             |
| GPU                 | Dual NVIDIA RTX 5060 Ti (16 GB VRAM each)  |
| Storage (fast)      | `/dev/nvme1n1p1` ‚Üí Mounted as `/mnt/nvme0n1` (3.6‚ÄØTB) |
| Storage (archive)   | `/dev/sda1` ‚Üí Mounted as `/mnt/sda` (7.3‚ÄØTB)       |
| Network IP          | `192.168.10.27`                            |
| Hostname            | `hx-llm-server-01`                         |
| DNS / Gateway       | Static internal DNS / 192.168.10.1         |

---

## 3. üß∞ OS & Base System Setup

- **OS**: Ubuntu Server 24.04 LTS (minimal install)
- **Kernel**: 6.8.x-generic
- **Hostname Configuration**:
  ```bash
  hostnamectl set-hostname hx-llm-server-01
  ```

- **APT Repository Enhancements**:
  ```bash
  add-apt-repository universe
  add-apt-repository multiverse
  apt update && apt upgrade -y
  ```

---

## 4. üß† GPU & Driver Stack

### 4.1 NVIDIA Driver

- **Version**: `nvidia-driver-575-open`
- **Installation Source**: Local `.run` package (from mounted volume)
- **Validation**:
  ```bash
  nvidia-smi
  ```

### 4.2 CUDA Toolkit

- **Version**: 12.9
- **Install Path**: `/usr/local/cuda-12.9`
- **Environment Vars** (added to `/etc/profile.d/cuda.sh`):
  ```bash
  export PATH=/usr/local/cuda-12.9/bin:$PATH
  export LD_LIBRARY_PATH=/usr/local/cuda-12.9/lib64:$LD_LIBRARY_PATH
  ```

### 4.3 cuDNN

- **Version**: Compatible with CUDA 12.9 (manual tar install)
- **Libraries Verified**:
  ```bash
  ls /usr/local/cuda-12.9/lib64/libcudnn*
  ```

---

## 5. üêç Python & AI Stack

### 5.1 Python

- **Version**: Python 3.12.3
- **Virtualenv**: `/opt/citadel/env`
- **Setup**:
  ```bash
  sudo python3.12 -m venv /opt/citadel/env
  source /opt/citadel/env/bin/activate
  ```

### 5.2 Core Packages Installed

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129
pip install vllm transformers accelerate
pip install fastapi uvicorn pyyaml psutil rich
```

---

## 6. üìÅ Storage & Filesystem Structure

```bash
# Model Storage (fast NVMe)
mount /dev/nvme1n1p1 /mnt/nvme0n1

# Log and Backup Storage (spinning disk)
mount /dev/sda1 /mnt/sda
```

### Model Directory Layout

```
/mnt/nvme0n1/models/
‚îú‚îÄ‚îÄ imp-v1-3b/
‚îú‚îÄ‚îÄ Nous-Hermes-2-Mixtral-8x7B-DPO/
‚îú‚îÄ‚îÄ phi-3-mini-4k-instruct/
‚îú‚îÄ‚îÄ Qwen-Coder-DeepSeek-R1-14B/
```

### Logs Directory

```
/mnt/sda/logs/llm/
‚îú‚îÄ‚îÄ vllm/
‚îú‚îÄ‚îÄ inference/
‚îú‚îÄ‚îÄ gpu-metrics/
```

---

## 7. üß† vLLM Runtime Configuration

### 7.1 Runtime Engine

- **Runtime**: vLLM
- **Version**: Latest pip install as of July 2025
- **Startup Script**: `/opt/citadel/bin/start_vllm.sh`

### 7.2 Model Launch Example

```bash
CUDA_VISIBLE_DEVICES=0,1 \
python3 -m vllm.entrypoints.api_server \
  --model /mnt/nvme0n1/models/Nous-Hermes-2-Mixtral-8x7B-DPO \
  --tokenizer /mnt/nvme0n1/models/Nous-Hermes-2-Mixtral-8x7B-DPO \
  --port 8001 \
  --host 0.0.0.0 \
  --max-num-batched-tokens 32768 \
  --max-model-len 32768 \
  --gpu-memory-utilization 0.90 \
  --swap-space 8
```

---

## 8. üîê Security & Access Control

- **SSH Access**: Public key-based only, root login disabled
- **UFW Rules**:
  ```bash
  ufw allow 22/tcp
  ufw allow 8000:8100/tcp
  ufw default deny incoming
  ufw enable
  ```

- **User Account**:
  - `citadel` (sudoer, owns runtime processes)
  - Home: `/home/citadel`

- **Secrets Management**:
  - Mounted secrets vault at `/opt/citadel/secrets/`
  - Used for `hf_token`, `pgpass`, and secure launch credentials

---

## 9. üì¶ Services & Cron Jobs

### Systemd Service (vLLM API Server)

```ini
[Unit]
Description=Citadel vLLM API Server
After=network.target

[Service]
Type=simple
User=citadel
WorkingDirectory=/opt/citadel
ExecStart=/opt/citadel/env/bin/python -m vllm.entrypoints.api_server ...
Restart=always

[Install]
WantedBy=multi-user.target
```

### Cron Job: GPU Temp & Memory Logger

```bash
*/5 * * * * /opt/citadel/bin/gpu_monitor.sh >> /mnt/sda/logs/llm/gpu-metrics/gpu.log
```

---

## üîç 10. Validation Checklist

- [x] `nvidia-smi` shows 2x RTX 5060 Ti with full utilization
- [x] `/mnt/nvme0n1/models` is readable with proper model folders
- [x] `curl localhost:8001/v1/completions` returns valid JSON
- [x] `/mnt/sda/logs/llm/` contains real-time logging output
- [x] `systemctl status vllm.service` reports `active (running)`
- [x] Models return valid completions using known prompts

---

## ‚úÖ Status Summary

| Component          | Status       | Notes                                    |
|-------------------|--------------|------------------------------------------|
| OS + Drivers       | ‚úÖ Complete   | Stable base image                        |
| CUDA + cuDNN       | ‚úÖ Complete   | Matched to GPU + PyTorch stack           |
| Python Env         | ‚úÖ Complete   | Virtualenv + packages tested             |
| Model Storage      | ‚úÖ Mounted    | Fast NVMe + archived storage split       |
| vLLM API Runtime   | ‚úÖ Active     | Systemd-managed and externally reachable |
| Logging + Metrics  | ‚úÖ Running    | Cron-logged GPU monitoring               |

---

## üìò Appendices

- **Model Versions Deployed**:
  - `MILVLG/imp-v1-3b`
  - `NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO`
  - `microsoft/Phi-3-mini-4k-instruct`
  - `radna/Qwen-Coder-DeepSeek-R1-14B`

- **Next Node for Replication**: `hx-llm-server-02` (currently in hardware prep)

---

¬© 2025 Hana-X | Citadel Infrastructure Division  
Document: `HX-LLM-SERVER-01_CONFIG_V1.0.0.md`